---
layout: post
title: "数据库相关命令"
subtitle: "DP实习时总结的一些命令"
author: "guanlili"
header-img: "img/blog-cover-database.jpg"
header-img-credit: "@WebdesignerDepot"
header-img-credit-href: "medium.com/@WebdesignerDepot/poll-should-css-become-more-like-a-programming-language-c74eb26a4270"
header-mask: 0.4
tags:
  - linux
  - dp实习
  - 数据库
---

# **各种数据库相关知识**

## **oracle相关**

oralce将view转成table
create table xxx as select * from "viewxxxx";

## **hive相关**

### **hive查询**

ssh dp@127.0.0.1  连接到hive服务器如cdh01

[dp@cdh01 ~]$ sudo -i

[root@cdh01 ~]# beeline

beeline> !connect jdbc://cdh03:10000

hdfs

或 

1.ssh cdh03

2.hive (如果没权限，执行: sudo -u hdfs hive)

我一般用后者。

show databases;显示所有的数据库：

use hive_orc;选择数据库orc；

show tables;查看数据库中所有的表单；

select * from gll31;查询表单数据；

### **Hive 增删改查相关操作**

修改表字段

 ALTER TABLE 表名 CHANGE 旧字段 新字段 类型;

 ALTER TABLE yzy_0612_copy  CHANGE a aa int;

添加字段
ALTER TABLE test ADD COLUMNS(age Int);

创建测试表
CREATE TABLE IF NOT EXISTS test (id BIGINT, name STRING);

插入一条数据
INSERT INTO TABLE test VALUES(2341344423,"lisi");


查看表的字段信息

desc yzy_0612_copy;

删除字段(使用新schema替换原有的)

ALTER TABLE yzy_0612_copy REPLACE COLUMNS(id BIGINT, name STRING);

### **Hive分区添加增量数据**

1、先把文件准备好 然后从要读取的路径外面将文件copy 到读取的路径

2、以hdfs用户身份操作

3、命令：[root@cdh03 ~]# sudo -u hdfs hdfs dfs -cp 原文件 目标文件



实际操作 先在cdh03下通过命令查看查看orc目录下的数据

[dp@cdh03 ~]$ hdfs dfs -ls /data/hive_orc/ss1

显示数据

-rw-r--r--  3 hdfs supergroup    3508 2019-10-29 03:01 /data/hdfs_parquet/ss1/dptask_48.hdfsFile.ss1_ss1+0+0000000000+00000000011.parquet

复制数据的目录修改名字如

[root@cdh03 ~]# sudo -u hdfs hdfs dfs -cp /data/hive_orc/ss1/dptask_28.hdfsFile.ss1_ss1+0+0000000000+0000000009.orc /data/hive_orc/ss1/dptask_28.hdfsFile.ss1_ss1+0+0000000001+00000000011.orc

就相当于把ss1中的文件copy了一条增量

再查就多了一条数据添加增量成功

 [dp@cdh03 ~]$ hdfs dfs -ls /data/hive_orc/ss1

### **如何查看hive 源中的表是否分区**

ssh 连接服务器

sudo -i 获取权限

ssh cdh03连接到03 

hive 进入hive

use hive_parquet;

show tables;

select * from gll31;查找目的表

show create table gll31;

show partitions gll31;有一堆id就是分区了的

## **kafka相关**

kafka 127.0.0.1

kafka的另类之处在于，**只能写入已有的topic**，也就是说需要提前创建一个新的topic才可以写，

localhost:~ yuzhenyu$ ssh dp@127.0.0.1 //先连接到Kafka服务器

[dp@kafka ~]$ sudo -i

### **创建Kafka topic** 

### **查看kafka目的地数据**

[root@kafka ~]# cd /data/kafka/bin

[root@kafka bin]# ./kafka-console-consumer.sh --zookeeper localhost:8888 --topic yzy_03 --from-beginning


### **kafka目的地创建topic:** 

./kafka-topics.sh --create --zookeeper localhost:8888 --replication-factor 1 --partitions 1 --topic gll_01

### **kafka目的地消费数据：**

./kafka-console-consumer.sh  --zookeeper localhost:8888 --topic gll_01 --from-beginning

### **kafka目的地验证一致性：**
验证一致性：
就是说Kafka消费了多少数据，目的地就有多少数据

看看kafka里的消费数据和目的地是否一致

是我们的kafka topic 和目的地的kafka 数据量一致

目的地数据量：

[root@kafka bin]#  ./kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list localhost:9999 --topic gll_13 --time -1 --offsets 1

gll_13:0:1000000

查消费：

你在 singlehost 目录下
执行 ../topic.sh list
然后根据 dptask id 找到一条记录
再用这个记录去 ../topic describe 这个记录

[root@zhenyu-test ~]# cd 2.7/datapipeline/singlehost/

[root@zhenyu-test singlehost]# ../topic.sh list

task_operation_sinkdp1
task
task_operation_sourcedp1
error_queue
connect-dp-ftp-sink-connector-dptask_1_8
task_operation_null
connect-dp-kafka08-connector-dptask_5_7
connect-dp-kafka08-connector-dptask_2_7
connect-dp-kafka08-connector-dptask_3_7

[root@zhenyu-test singlehost]# ../topic.sh describe 
connect-dp-kafka08-connector-dptask_5_7

[root@zhenyu-test singlehost]# ../topic.sh desc connect-dp-kafka08-connector-dptask_5_7

TOPIC                                              PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID                                                                               HOST            CLIENT-ID
v2_dptask_2.dp_test.A_Pk_table_inc100gll_copy4.989 0          2000007         2000008         1               connector-consumer-dp-kafka08-connector-dptask_2_7-0-3866b8b4-9733-47f2-8ad6-c8759fb68ec0 /192.168.112.10 connector-consumer-dp-kafka08-connector-dptask_2_7-0

2000008 就是 我们的kafka topic 数据量

绕过前端重启容器

sinkdp容器号  654be3061da5

source容器号  368e60bed7b0

 docker exec -it 654be3061da5 curl -XPUT manager:2222/dptasks/5/restart

 docker exec -it 368e60bed7b0 curl -XPUT manager:2222/dptasks/5/restart


## **验证hive kerberos**


1.认证方式 选择Kerberos；上传krb5.conf配置；HDFS Principal：hdfs/kerberos-cdh@DP.COM；上传HDFS Keytab配置	
2.配置sinkdp，manager，webservice的host映射
- 微软云:
kerberos.dp.com:127.0.0.1
kerberos-cdh:127.0.0.1

（web service里的extra——host 和hosts只能加一个 不能两个都加。否则出现 测试HDFS读写失败。报错。
## **hdfs相关**

### **hdfs读取分区查询**

步骤：

1、ssh dp@127.0.0.1

2、sudo -i

3、ssh cdh03

4、 hdfs dfs -ls /data/hdfs_lzo/ss1_fenqu

(注意表名和目录，目录去任务目的地看)

## **Hbase 相关**

### **Hbase目的地**

ssh dp@127.0.0.1

sudo -i

hbase shell

list

scan '表名'

高级清洗：

record.put("rowkey",record.get("id")+"5");（拼接两个字符串）



## **ftp相关**

### **ftp数据源**

ssh dp@127.0.0.1

sudo -i

cd /data/nftp/

cd source/

cd static/

rz上传命令不会用，

http://fs.datapipeline.com:7777/fs/mnt/fs/csv/

可传到这个网址里，然后wget这个地址
### **ftp目的地**


ssh dp@127.0.0.1

sudo -i

cd /data/nftp/

cd sink/

cd 数字目录 网址上的序号

cd ss1/

 cat 0-9 
